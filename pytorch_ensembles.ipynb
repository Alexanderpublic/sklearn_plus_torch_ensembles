{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch_ensembles.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2ulRVpuY2EU",
        "colab_type": "text"
      },
      "source": [
        "# reading and viewing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peEdXQ_vXZoe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import load_digits\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwuM8QHXXtNx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x, y = load_digits(n_class=10, return_X_y=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JTnmMczYNht",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4f1bfdee-d7d7-4078-d851-d47632d70378"
      },
      "source": [
        "x.shape"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1797, 64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6gDLXG-X_9Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "outputId": "ca7c9104-cd39-468b-aa64-0523c44edbe3"
      },
      "source": [
        "index = 0\n",
        "\n",
        "print(y[index])\n",
        "plt.imshow(x[index].reshape(8, 8), cmap=\"Greys\")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f5b1e883e80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPUAAAD4CAYAAAA0L6C7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypPhdg5vYLCB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "23345f26-4331-4281-c9a9-accb188f65a3"
      },
      "source": [
        "x[index]"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.,  0.,  5., 13.,  9.,  1.,  0.,  0.,  0.,  0., 13., 15., 10.,\n",
              "       15.,  5.,  0.,  0.,  3., 15.,  2.,  0., 11.,  8.,  0.,  0.,  4.,\n",
              "       12.,  0.,  0.,  8.,  8.,  0.,  0.,  5.,  8.,  0.,  0.,  9.,  8.,\n",
              "        0.,  0.,  4., 11.,  0.,  1., 12.,  7.,  0.,  0.,  2., 14.,  5.,\n",
              "       10., 12.,  0.,  0.,  0.,  0.,  6., 13., 10.,  0.,  0.,  0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VY3zydxaRtm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "153c4d4b-0c2e-489a-c501-b527f9726cb0"
      },
      "source": [
        "np.unique(y)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ec7AMMGeY41u",
        "colab_type": "text"
      },
      "source": [
        "# preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoZOPN_hYfvx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ml-4KjHYzEA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2,\n",
        "                                                    shuffle=True, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zy9ERHTgY_1d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "127bf765-e294-4375-d28f-ef2d52246e7c"
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1437, 64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7SysyO4BI04",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "894119be-02e1-482f-b793-c9f56002c31a"
      },
      "source": [
        "x_test.shape"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(360, 64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndl4tdMEZLHI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6fc29864-33ff-456e-9edd-f87f255d00dd"
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1437,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIZIp40mZLr8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "de661b7e-c835-422e-c336-b880e507c43b"
      },
      "source": [
        "scaler = StandardScaler()\n",
        "scaler.fit(x_train)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StandardScaler(copy=True, with_mean=True, with_std=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbCyHKjZZSEi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train_scaled = scaler.transform(x_train)\n",
        "x_test_scaled = scaler.transform(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GF7xIfnjbDtg",
        "colab_type": "text"
      },
      "source": [
        "# building a model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBYcckWDbHcr",
        "colab_type": "text"
      },
      "source": [
        "## pytorch CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1EbJEdsajhG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhfrCV94cJt5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(SimpleCNN, self).__init__()\n",
        "\n",
        "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=5, stride=1, padding=2)\n",
        "    self.conv1_s = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=5, stride=2, padding=2)\n",
        "    self.conv2 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=3, stride=1, padding=1)\n",
        "    self.conv2_s = nn.Conv2d(in_channels=6, out_channels=6, kernel_size=3, stride=2, padding=1)\n",
        "    self.conv3 = nn.Conv2d(in_channels=6, out_channels=10, kernel_size=3, stride=1, padding=1)\n",
        "    self.conv3_s = nn.Conv2d(in_channels=10, out_channels=10, kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.fc1 = nn.Linear(10, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.conv1(x))\n",
        "    x = F.relu(self.conv1_s(x))\n",
        "    x = F.relu(self.conv2(x))\n",
        "    x = F.relu(self.conv2_s(x))\n",
        "    x = F.relu(self.conv3(x))\n",
        "    x = F.relu(self.conv3_s(x))\n",
        "\n",
        "    x = self.flatten(x)\n",
        "    x = self.fc1(x)\n",
        "    x = F.softmax(x)\n",
        "\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U94WJWM-dy7O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 64\n",
        "learning_rate = 1e-3\n",
        "epochs = 200"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unD0NdTGeeDK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train_tensor = torch.tensor(x_train_scaled.reshape(-1, 1, 8, 8).astype(np.float32))\n",
        "x_test_tensor = torch.tensor(x_test_scaled.reshape(-1, 1, 8, 8).astype(np.float32))\n",
        "\n",
        "y_train_tensor = torch.tensor(y_train.astype(np.long))\n",
        "y_test_tensor = torch.tensor(y_test.astype(np.long))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfL8Oq6od71t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
        "\n",
        "test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0NZoLMUe1Jv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "simple_cnn = SimpleCNN().cuda()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = Adam(simple_cnn.parameters(), lr=learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McgUaPwKdVPK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "408da683-9218-49ab-b2bb-a6ab461b09ef"
      },
      "source": [
        "for epoch in range(epochs):\n",
        "  simple_cnn.train()\n",
        "  train_samples_count = 0\n",
        "  true_train_samples_count = 0\n",
        "  running_loss = 0\n",
        "\n",
        "  for batch in train_loader:\n",
        "    x_data = batch[0].cuda()\n",
        "    y_data = batch[1].cuda()\n",
        "\n",
        "    y_pred = simple_cnn(x_data)\n",
        "    loss = criterion(y_pred, y_data)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "\n",
        "    y_pred = y_pred.argmax(dim=1, keepdim=False)\n",
        "    true_classified = (y_pred == y_data).sum().item()\n",
        "    true_train_samples_count += true_classified\n",
        "    train_samples_count += len(x_data)\n",
        "  \n",
        "  train_accuracy = true_train_samples_count / train_samples_count\n",
        "  print(f\"[{epoch}] train loss: {running_loss}, accuracy: {round(train_accuracy, 4)}\")\n",
        "\n",
        "  simple_cnn.eval()\n",
        "  test_samples_count = 0\n",
        "  true_test_samples_count = 0\n",
        "  running_loss = 0\n",
        "\n",
        "  for batch in test_loader:\n",
        "    x_data = batch[0].cuda()\n",
        "    y_data = batch[1].cuda()\n",
        "\n",
        "    y_pred = simple_cnn(x_data)\n",
        "    loss = criterion(y_pred, y_data)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "\n",
        "    y_pred = y_pred.argmax(dim=1, keepdim=False)\n",
        "    true_classified = (y_pred == y_data).sum().item()\n",
        "    true_test_samples_count += true_classified\n",
        "    test_samples_count += len(x_data)\n",
        "  \n",
        "  test_accuracy = true_test_samples_count / test_samples_count\n",
        "  print(f\"[{epoch}] test loss: {running_loss}, accuracy: {round(test_accuracy, 4)}\")"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[0] train loss: 52.965962171554565, accuracy: 0.1016\n",
            "[0] test loss: 13.812444686889648, accuracy: 0.0972\n",
            "[1] train loss: 52.959094524383545, accuracy: 0.1016\n",
            "[1] test loss: 13.810017824172974, accuracy: 0.0972\n",
            "[2] train loss: 52.924052000045776, accuracy: 0.1065\n",
            "[2] test loss: 13.791139125823975, accuracy: 0.1694\n",
            "[3] train loss: 52.58355736732483, accuracy: 0.2317\n",
            "[3] test loss: 13.570852279663086, accuracy: 0.1861\n",
            "[4] train loss: 50.539629220962524, accuracy: 0.2276\n",
            "[4] test loss: 12.984268188476562, accuracy: 0.325\n",
            "[5] train loss: 48.47829580307007, accuracy: 0.373\n",
            "[5] test loss: 12.617634296417236, accuracy: 0.3333\n",
            "[6] train loss: 46.96195960044861, accuracy: 0.4154\n",
            "[6] test loss: 12.29778242111206, accuracy: 0.4083\n",
            "[7] train loss: 45.903536796569824, accuracy: 0.4711\n",
            "[7] test loss: 12.03239119052887, accuracy: 0.4917\n",
            "[8] train loss: 44.97504949569702, accuracy: 0.5268\n",
            "[8] test loss: 11.841533660888672, accuracy: 0.5028\n",
            "[9] train loss: 44.48520493507385, accuracy: 0.54\n",
            "[9] test loss: 11.723801136016846, accuracy: 0.525\n",
            "[10] train loss: 44.01883339881897, accuracy: 0.5685\n",
            "[10] test loss: 11.629214406013489, accuracy: 0.55\n",
            "[11] train loss: 43.48373067378998, accuracy: 0.5825\n",
            "[11] test loss: 11.50286066532135, accuracy: 0.5556\n",
            "[12] train loss: 43.09919893741608, accuracy: 0.5971\n",
            "[12] test loss: 11.445982336997986, accuracy: 0.5639\n",
            "[13] train loss: 42.86431288719177, accuracy: 0.6033\n",
            "[13] test loss: 11.38118326663971, accuracy: 0.5722\n",
            "[14] train loss: 42.68081998825073, accuracy: 0.6096\n",
            "[14] test loss: 11.33737576007843, accuracy: 0.5778\n",
            "[15] train loss: 42.57274878025055, accuracy: 0.6124\n",
            "[15] test loss: 11.338703632354736, accuracy: 0.5806\n",
            "[16] train loss: 42.40012204647064, accuracy: 0.6221\n",
            "[16] test loss: 11.328304171562195, accuracy: 0.5833\n",
            "[17] train loss: 42.228156208992004, accuracy: 0.6298\n",
            "[17] test loss: 11.28107762336731, accuracy: 0.5944\n",
            "[18] train loss: 42.10826301574707, accuracy: 0.6354\n",
            "[18] test loss: 11.222456216812134, accuracy: 0.6056\n",
            "[19] train loss: 42.03979170322418, accuracy: 0.6381\n",
            "[19] test loss: 11.180649280548096, accuracy: 0.6083\n",
            "[20] train loss: 41.99254930019379, accuracy: 0.6409\n",
            "[20] test loss: 11.150348663330078, accuracy: 0.6139\n",
            "[21] train loss: 41.95044004917145, accuracy: 0.643\n",
            "[21] test loss: 11.126753211021423, accuracy: 0.6167\n",
            "[22] train loss: 41.9307667016983, accuracy: 0.6437\n",
            "[22] test loss: 11.112998485565186, accuracy: 0.6222\n",
            "[23] train loss: 41.98029971122742, accuracy: 0.6409\n",
            "[23] test loss: 11.130540013313293, accuracy: 0.6167\n",
            "[24] train loss: 42.05722510814667, accuracy: 0.6381\n",
            "[24] test loss: 11.192769527435303, accuracy: 0.6056\n",
            "[25] train loss: 42.03430736064911, accuracy: 0.6347\n",
            "[25] test loss: 11.155330538749695, accuracy: 0.6028\n",
            "[26] train loss: 41.82598423957825, accuracy: 0.6451\n",
            "[26] test loss: 11.032167434692383, accuracy: 0.625\n",
            "[27] train loss: 41.5835177898407, accuracy: 0.6555\n",
            "[27] test loss: 11.02794897556305, accuracy: 0.625\n",
            "[28] train loss: 41.509828090667725, accuracy: 0.6604\n",
            "[28] test loss: 11.030222535133362, accuracy: 0.625\n",
            "[29] train loss: 41.47819721698761, accuracy: 0.6618\n",
            "[29] test loss: 11.021936893463135, accuracy: 0.625\n",
            "[30] train loss: 41.45686101913452, accuracy: 0.6618\n",
            "[30] test loss: 11.013878345489502, accuracy: 0.6278\n",
            "[31] train loss: 41.436715722084045, accuracy: 0.6625\n",
            "[31] test loss: 11.009105682373047, accuracy: 0.6278\n",
            "[32] train loss: 41.425745844841, accuracy: 0.6625\n",
            "[32] test loss: 11.007428646087646, accuracy: 0.6278\n",
            "[33] train loss: 41.41409158706665, accuracy: 0.6632\n",
            "[33] test loss: 11.00425100326538, accuracy: 0.6333\n",
            "[34] train loss: 41.401349902153015, accuracy: 0.6625\n",
            "[34] test loss: 10.99935781955719, accuracy: 0.6333\n",
            "[35] train loss: 41.39406931400299, accuracy: 0.6632\n",
            "[35] test loss: 10.990265607833862, accuracy: 0.6333\n",
            "[36] train loss: 41.36861824989319, accuracy: 0.6625\n",
            "[36] test loss: 10.973936557769775, accuracy: 0.6306\n",
            "[37] train loss: 41.31966245174408, accuracy: 0.6646\n",
            "[37] test loss: 10.95758593082428, accuracy: 0.6306\n",
            "[38] train loss: 41.2785964012146, accuracy: 0.6688\n",
            "[38] test loss: 10.95236873626709, accuracy: 0.6333\n",
            "[39] train loss: 41.25807988643646, accuracy: 0.6688\n",
            "[39] test loss: 10.965147256851196, accuracy: 0.6333\n",
            "[40] train loss: 41.24570107460022, accuracy: 0.6695\n",
            "[40] test loss: 10.958587408065796, accuracy: 0.6361\n",
            "[41] train loss: 41.234464049339294, accuracy: 0.6695\n",
            "[41] test loss: 10.956449627876282, accuracy: 0.6361\n",
            "[42] train loss: 41.22639453411102, accuracy: 0.6695\n",
            "[42] test loss: 10.95403516292572, accuracy: 0.6389\n",
            "[43] train loss: 41.218679428100586, accuracy: 0.6695\n",
            "[43] test loss: 10.950200915336609, accuracy: 0.6389\n",
            "[44] train loss: 41.20992434024811, accuracy: 0.6695\n",
            "[44] test loss: 10.946092128753662, accuracy: 0.6389\n",
            "[45] train loss: 41.20183229446411, accuracy: 0.6695\n",
            "[45] test loss: 10.94472086429596, accuracy: 0.6389\n",
            "[46] train loss: 41.19619560241699, accuracy: 0.6695\n",
            "[46] test loss: 10.946650862693787, accuracy: 0.6389\n",
            "[47] train loss: 41.189937710762024, accuracy: 0.6695\n",
            "[47] test loss: 10.94900643825531, accuracy: 0.6389\n",
            "[48] train loss: 41.18439471721649, accuracy: 0.6701\n",
            "[48] test loss: 10.950886249542236, accuracy: 0.6389\n",
            "[49] train loss: 41.17768180370331, accuracy: 0.6701\n",
            "[49] test loss: 10.955844044685364, accuracy: 0.6389\n",
            "[50] train loss: 41.17053163051605, accuracy: 0.6715\n",
            "[50] test loss: 10.95589029788971, accuracy: 0.6389\n",
            "[51] train loss: 41.17423331737518, accuracy: 0.6715\n",
            "[51] test loss: 10.961453080177307, accuracy: 0.6333\n",
            "[52] train loss: 41.14401876926422, accuracy: 0.675\n",
            "[52] test loss: 10.986799240112305, accuracy: 0.6333\n",
            "[53] train loss: 41.10889136791229, accuracy: 0.6771\n",
            "[53] test loss: 11.006724238395691, accuracy: 0.6333\n",
            "[54] train loss: 41.14408266544342, accuracy: 0.6757\n",
            "[54] test loss: 11.000786900520325, accuracy: 0.6306\n",
            "[55] train loss: 41.15170443058014, accuracy: 0.675\n",
            "[55] test loss: 10.992266774177551, accuracy: 0.6306\n",
            "[56] train loss: 41.208401918411255, accuracy: 0.6715\n",
            "[56] test loss: 10.974520564079285, accuracy: 0.6389\n",
            "[57] train loss: 41.18895614147186, accuracy: 0.6715\n",
            "[57] test loss: 10.965948700904846, accuracy: 0.6333\n",
            "[58] train loss: 41.15530705451965, accuracy: 0.6729\n",
            "[58] test loss: 10.93495798110962, accuracy: 0.6389\n",
            "[59] train loss: 41.13761866092682, accuracy: 0.6736\n",
            "[59] test loss: 10.913683533668518, accuracy: 0.6472\n",
            "[60] train loss: 41.0586861371994, accuracy: 0.6771\n",
            "[60] test loss: 10.892336368560791, accuracy: 0.65\n",
            "[61] train loss: 41.002933621406555, accuracy: 0.6785\n",
            "[61] test loss: 10.91261112689972, accuracy: 0.6417\n",
            "[62] train loss: 40.97729253768921, accuracy: 0.6806\n",
            "[62] test loss: 10.90799069404602, accuracy: 0.6444\n",
            "[63] train loss: 40.96941816806793, accuracy: 0.6799\n",
            "[63] test loss: 10.869439721107483, accuracy: 0.6528\n",
            "[64] train loss: 40.95284914970398, accuracy: 0.6799\n",
            "[64] test loss: 10.905094861984253, accuracy: 0.6444\n",
            "[65] train loss: 40.93238806724548, accuracy: 0.682\n",
            "[65] test loss: 10.871295809745789, accuracy: 0.6528\n",
            "[66] train loss: 40.91861438751221, accuracy: 0.682\n",
            "[66] test loss: 10.863029956817627, accuracy: 0.6528\n",
            "[67] train loss: 40.91007733345032, accuracy: 0.682\n",
            "[67] test loss: 10.872292876243591, accuracy: 0.6528\n",
            "[68] train loss: 40.89294159412384, accuracy: 0.6848\n",
            "[68] test loss: 10.88552701473236, accuracy: 0.65\n",
            "[69] train loss: 40.885751247406006, accuracy: 0.6848\n",
            "[69] test loss: 10.860709071159363, accuracy: 0.6556\n",
            "[70] train loss: 40.88021421432495, accuracy: 0.6848\n",
            "[70] test loss: 10.850616693496704, accuracy: 0.6583\n",
            "[71] train loss: 40.87490630149841, accuracy: 0.6848\n",
            "[71] test loss: 10.843337297439575, accuracy: 0.6611\n",
            "[72] train loss: 40.866105794906616, accuracy: 0.6848\n",
            "[72] test loss: 10.848731994628906, accuracy: 0.6556\n",
            "[73] train loss: 40.855557441711426, accuracy: 0.6848\n",
            "[73] test loss: 10.839491963386536, accuracy: 0.6611\n",
            "[74] train loss: 40.85548794269562, accuracy: 0.6848\n",
            "[74] test loss: 10.842020153999329, accuracy: 0.6583\n",
            "[75] train loss: 40.84767234325409, accuracy: 0.6848\n",
            "[75] test loss: 10.834358930587769, accuracy: 0.6611\n",
            "[76] train loss: 40.84984803199768, accuracy: 0.6848\n",
            "[76] test loss: 10.82943081855774, accuracy: 0.6611\n",
            "[77] train loss: 40.84275197982788, accuracy: 0.6848\n",
            "[77] test loss: 10.828344464302063, accuracy: 0.6611\n",
            "[78] train loss: 40.8376659154892, accuracy: 0.6855\n",
            "[78] test loss: 10.828115820884705, accuracy: 0.6611\n",
            "[79] train loss: 40.80949258804321, accuracy: 0.6875\n",
            "[79] test loss: 10.827392578125, accuracy: 0.6611\n",
            "[80] train loss: 40.780508399009705, accuracy: 0.6882\n",
            "[80] test loss: 10.829541563987732, accuracy: 0.6639\n",
            "[81] train loss: 40.77735149860382, accuracy: 0.6882\n",
            "[81] test loss: 10.82584547996521, accuracy: 0.6611\n",
            "[82] train loss: 40.78220534324646, accuracy: 0.6882\n",
            "[82] test loss: 10.829825639724731, accuracy: 0.6611\n",
            "[83] train loss: 40.77195382118225, accuracy: 0.6896\n",
            "[83] test loss: 10.833168983459473, accuracy: 0.6639\n",
            "[84] train loss: 40.76497554779053, accuracy: 0.6896\n",
            "[84] test loss: 10.839226603507996, accuracy: 0.6583\n",
            "[85] train loss: 40.77540636062622, accuracy: 0.6896\n",
            "[85] test loss: 10.853673934936523, accuracy: 0.65\n",
            "[86] train loss: 40.76725912094116, accuracy: 0.6896\n",
            "[86] test loss: 10.860660195350647, accuracy: 0.65\n",
            "[87] train loss: 40.758259415626526, accuracy: 0.6896\n",
            "[87] test loss: 10.835317134857178, accuracy: 0.6556\n",
            "[88] train loss: 40.74062979221344, accuracy: 0.6896\n",
            "[88] test loss: 10.871988534927368, accuracy: 0.6528\n",
            "[89] train loss: 40.74246859550476, accuracy: 0.6903\n",
            "[89] test loss: 10.822132468223572, accuracy: 0.6583\n",
            "[90] train loss: 40.7247154712677, accuracy: 0.6903\n",
            "[90] test loss: 10.812775373458862, accuracy: 0.6667\n",
            "[91] train loss: 40.70750629901886, accuracy: 0.6903\n",
            "[91] test loss: 10.82832407951355, accuracy: 0.6611\n",
            "[92] train loss: 40.70202362537384, accuracy: 0.6903\n",
            "[92] test loss: 10.82328450679779, accuracy: 0.6639\n",
            "[93] train loss: 40.69728076457977, accuracy: 0.691\n",
            "[93] test loss: 10.815413355827332, accuracy: 0.6611\n",
            "[94] train loss: 40.68763244152069, accuracy: 0.6917\n",
            "[94] test loss: 10.813496589660645, accuracy: 0.6611\n",
            "[95] train loss: 40.68002700805664, accuracy: 0.6917\n",
            "[95] test loss: 10.819448947906494, accuracy: 0.6639\n",
            "[96] train loss: 40.67815089225769, accuracy: 0.6917\n",
            "[96] test loss: 10.820869207382202, accuracy: 0.6611\n",
            "[97] train loss: 40.677483320236206, accuracy: 0.6917\n",
            "[97] test loss: 10.811613917350769, accuracy: 0.6611\n",
            "[98] train loss: 40.67465436458588, accuracy: 0.6917\n",
            "[98] test loss: 10.815248131752014, accuracy: 0.6639\n",
            "[99] train loss: 40.67023313045502, accuracy: 0.6917\n",
            "[99] test loss: 10.817256450653076, accuracy: 0.6667\n",
            "[100] train loss: 40.66867673397064, accuracy: 0.6917\n",
            "[100] test loss: 10.818976044654846, accuracy: 0.6639\n",
            "[101] train loss: 40.66926670074463, accuracy: 0.6917\n",
            "[101] test loss: 10.822098135948181, accuracy: 0.6639\n",
            "[102] train loss: 40.66900992393494, accuracy: 0.6917\n",
            "[102] test loss: 10.819762110710144, accuracy: 0.6611\n",
            "[103] train loss: 40.66709506511688, accuracy: 0.6917\n",
            "[103] test loss: 10.818313956260681, accuracy: 0.6639\n",
            "[104] train loss: 40.662845849990845, accuracy: 0.6917\n",
            "[104] test loss: 10.819848775863647, accuracy: 0.6611\n",
            "[105] train loss: 40.65697371959686, accuracy: 0.6924\n",
            "[105] test loss: 10.821258306503296, accuracy: 0.6611\n",
            "[106] train loss: 40.64671564102173, accuracy: 0.6931\n",
            "[106] test loss: 10.816167712211609, accuracy: 0.6639\n",
            "[107] train loss: 40.65362560749054, accuracy: 0.6931\n",
            "[107] test loss: 10.808565139770508, accuracy: 0.6583\n",
            "[108] train loss: 40.66239035129547, accuracy: 0.6924\n",
            "[108] test loss: 10.801775217056274, accuracy: 0.6611\n",
            "[109] train loss: 40.66904282569885, accuracy: 0.6924\n",
            "[109] test loss: 10.799009442329407, accuracy: 0.6611\n",
            "[110] train loss: 40.674068331718445, accuracy: 0.6931\n",
            "[110] test loss: 10.798589944839478, accuracy: 0.6611\n",
            "[111] train loss: 40.63302457332611, accuracy: 0.6938\n",
            "[111] test loss: 10.797484517097473, accuracy: 0.6667\n",
            "[112] train loss: 40.63744628429413, accuracy: 0.6945\n",
            "[112] test loss: 10.828529953956604, accuracy: 0.6611\n",
            "[113] train loss: 40.629072308540344, accuracy: 0.6945\n",
            "[113] test loss: 10.812379956245422, accuracy: 0.6639\n",
            "[114] train loss: 40.64258325099945, accuracy: 0.6945\n",
            "[114] test loss: 10.798583745956421, accuracy: 0.6639\n",
            "[115] train loss: 40.628549456596375, accuracy: 0.6945\n",
            "[115] test loss: 10.800614833831787, accuracy: 0.6583\n",
            "[116] train loss: 40.63376557826996, accuracy: 0.6938\n",
            "[116] test loss: 10.81544542312622, accuracy: 0.6556\n",
            "[117] train loss: 40.617937088012695, accuracy: 0.6945\n",
            "[117] test loss: 10.849002003669739, accuracy: 0.65\n",
            "[118] train loss: 40.61980998516083, accuracy: 0.6945\n",
            "[118] test loss: 10.827373266220093, accuracy: 0.6583\n",
            "[119] train loss: 40.620508432388306, accuracy: 0.6938\n",
            "[119] test loss: 10.801382660865784, accuracy: 0.6611\n",
            "[120] train loss: 40.604442715644836, accuracy: 0.6945\n",
            "[120] test loss: 10.797287583351135, accuracy: 0.6639\n",
            "[121] train loss: 40.61304748058319, accuracy: 0.6945\n",
            "[121] test loss: 10.820390343666077, accuracy: 0.6583\n",
            "[122] train loss: 40.59999370574951, accuracy: 0.6945\n",
            "[122] test loss: 10.811290860176086, accuracy: 0.6611\n",
            "[123] train loss: 40.600783348083496, accuracy: 0.6945\n",
            "[123] test loss: 10.807570695877075, accuracy: 0.6556\n",
            "[124] train loss: 40.59587574005127, accuracy: 0.6945\n",
            "[124] test loss: 10.799779057502747, accuracy: 0.6639\n",
            "[125] train loss: 40.591747760772705, accuracy: 0.6945\n",
            "[125] test loss: 10.805168628692627, accuracy: 0.6583\n",
            "[126] train loss: 40.590827226638794, accuracy: 0.6945\n",
            "[126] test loss: 10.805905222892761, accuracy: 0.6583\n",
            "[127] train loss: 40.585540413856506, accuracy: 0.6945\n",
            "[127] test loss: 10.815033078193665, accuracy: 0.6611\n",
            "[128] train loss: 40.59240925312042, accuracy: 0.6945\n",
            "[128] test loss: 10.817758202552795, accuracy: 0.6583\n",
            "[129] train loss: 40.595818877220154, accuracy: 0.6952\n",
            "[129] test loss: 10.808277487754822, accuracy: 0.6611\n",
            "[130] train loss: 40.582911133766174, accuracy: 0.6952\n",
            "[130] test loss: 10.808898687362671, accuracy: 0.6583\n",
            "[131] train loss: 40.577473163604736, accuracy: 0.6952\n",
            "[131] test loss: 10.807618260383606, accuracy: 0.6583\n",
            "[132] train loss: 40.574283838272095, accuracy: 0.6952\n",
            "[132] test loss: 10.809587717056274, accuracy: 0.6611\n",
            "[133] train loss: 40.57452058792114, accuracy: 0.6952\n",
            "[133] test loss: 10.810282468795776, accuracy: 0.6611\n",
            "[134] train loss: 40.57434821128845, accuracy: 0.6952\n",
            "[134] test loss: 10.81339967250824, accuracy: 0.6611\n",
            "[135] train loss: 40.57582485675812, accuracy: 0.6952\n",
            "[135] test loss: 10.81599247455597, accuracy: 0.6583\n",
            "[136] train loss: 40.57682156562805, accuracy: 0.6952\n",
            "[136] test loss: 10.819133877754211, accuracy: 0.6556\n",
            "[137] train loss: 40.58036720752716, accuracy: 0.6952\n",
            "[137] test loss: 10.82564127445221, accuracy: 0.6556\n",
            "[138] train loss: 40.5803462266922, accuracy: 0.6952\n",
            "[138] test loss: 10.835866451263428, accuracy: 0.6528\n",
            "[139] train loss: 40.57776939868927, accuracy: 0.6952\n",
            "[139] test loss: 10.83677864074707, accuracy: 0.6528\n",
            "[140] train loss: 40.57070291042328, accuracy: 0.6952\n",
            "[140] test loss: 10.837745070457458, accuracy: 0.65\n",
            "[141] train loss: 40.57517910003662, accuracy: 0.6952\n",
            "[141] test loss: 10.831484079360962, accuracy: 0.6556\n",
            "[142] train loss: 40.573803424835205, accuracy: 0.6952\n",
            "[142] test loss: 10.814048171043396, accuracy: 0.6611\n",
            "[143] train loss: 40.56587755680084, accuracy: 0.6952\n",
            "[143] test loss: 10.828324675559998, accuracy: 0.6639\n",
            "[144] train loss: 40.55853796005249, accuracy: 0.6959\n",
            "[144] test loss: 10.826988816261292, accuracy: 0.6583\n",
            "[145] train loss: 40.563623547554016, accuracy: 0.6959\n",
            "[145] test loss: 10.815213918685913, accuracy: 0.6583\n",
            "[146] train loss: 40.55622053146362, accuracy: 0.6959\n",
            "[146] test loss: 10.800890326499939, accuracy: 0.6583\n",
            "[147] train loss: 40.55427277088165, accuracy: 0.6959\n",
            "[147] test loss: 10.799646139144897, accuracy: 0.6611\n",
            "[148] train loss: 40.55019569396973, accuracy: 0.6959\n",
            "[148] test loss: 10.801008701324463, accuracy: 0.6611\n",
            "[149] train loss: 40.54684400558472, accuracy: 0.6959\n",
            "[149] test loss: 10.806242942810059, accuracy: 0.6611\n",
            "[150] train loss: 40.54570019245148, accuracy: 0.6959\n",
            "[150] test loss: 10.81037712097168, accuracy: 0.6611\n",
            "[151] train loss: 40.5460889339447, accuracy: 0.6959\n",
            "[151] test loss: 10.813872694969177, accuracy: 0.6583\n",
            "[152] train loss: 40.54483413696289, accuracy: 0.6959\n",
            "[152] test loss: 10.813702583312988, accuracy: 0.6611\n",
            "[153] train loss: 40.54468071460724, accuracy: 0.6959\n",
            "[153] test loss: 10.81633472442627, accuracy: 0.6583\n",
            "[154] train loss: 40.54375410079956, accuracy: 0.6959\n",
            "[154] test loss: 10.81343138217926, accuracy: 0.6611\n",
            "[155] train loss: 40.54159188270569, accuracy: 0.6959\n",
            "[155] test loss: 10.816185116767883, accuracy: 0.6611\n",
            "[156] train loss: 40.541656613349915, accuracy: 0.6959\n",
            "[156] test loss: 10.815511465072632, accuracy: 0.6639\n",
            "[157] train loss: 40.54158699512482, accuracy: 0.6959\n",
            "[157] test loss: 10.816407322883606, accuracy: 0.6583\n",
            "[158] train loss: 40.54300844669342, accuracy: 0.6959\n",
            "[158] test loss: 10.815568923950195, accuracy: 0.6583\n",
            "[159] train loss: 40.543015360832214, accuracy: 0.6959\n",
            "[159] test loss: 10.815330624580383, accuracy: 0.6583\n",
            "[160] train loss: 40.54336404800415, accuracy: 0.6959\n",
            "[160] test loss: 10.815123200416565, accuracy: 0.6556\n",
            "[161] train loss: 40.54230999946594, accuracy: 0.6959\n",
            "[161] test loss: 10.813311100006104, accuracy: 0.6583\n",
            "[162] train loss: 40.54134929180145, accuracy: 0.6959\n",
            "[162] test loss: 10.81473970413208, accuracy: 0.6556\n",
            "[163] train loss: 40.53942286968231, accuracy: 0.6959\n",
            "[163] test loss: 10.814098596572876, accuracy: 0.6611\n",
            "[164] train loss: 40.53773069381714, accuracy: 0.6959\n",
            "[164] test loss: 10.81333613395691, accuracy: 0.6583\n",
            "[165] train loss: 40.534743905067444, accuracy: 0.6959\n",
            "[165] test loss: 10.813971638679504, accuracy: 0.6639\n",
            "[166] train loss: 40.53421485424042, accuracy: 0.6959\n",
            "[166] test loss: 10.808379888534546, accuracy: 0.6639\n",
            "[167] train loss: 40.53265655040741, accuracy: 0.6959\n",
            "[167] test loss: 10.79895532131195, accuracy: 0.6639\n",
            "[168] train loss: 40.53434681892395, accuracy: 0.6966\n",
            "[168] test loss: 10.81078851222992, accuracy: 0.6611\n",
            "[169] train loss: 40.53187441825867, accuracy: 0.6959\n",
            "[169] test loss: 10.797184228897095, accuracy: 0.6639\n",
            "[170] train loss: 40.535178780555725, accuracy: 0.6966\n",
            "[170] test loss: 10.798766851425171, accuracy: 0.6611\n",
            "[171] train loss: 40.53208243846893, accuracy: 0.6959\n",
            "[171] test loss: 10.795246005058289, accuracy: 0.6639\n",
            "[172] train loss: 40.5353661775589, accuracy: 0.6966\n",
            "[172] test loss: 10.788684368133545, accuracy: 0.6722\n",
            "[173] train loss: 40.53586745262146, accuracy: 0.6966\n",
            "[173] test loss: 10.785799860954285, accuracy: 0.6694\n",
            "[174] train loss: 40.53782498836517, accuracy: 0.6966\n",
            "[174] test loss: 10.793855428695679, accuracy: 0.6639\n",
            "[175] train loss: 40.53741991519928, accuracy: 0.6966\n",
            "[175] test loss: 10.792006373405457, accuracy: 0.6639\n",
            "[176] train loss: 40.53893840312958, accuracy: 0.6966\n",
            "[176] test loss: 10.810335397720337, accuracy: 0.6639\n",
            "[177] train loss: 40.53896462917328, accuracy: 0.6966\n",
            "[177] test loss: 10.8170245885849, accuracy: 0.6639\n",
            "[178] train loss: 40.53977823257446, accuracy: 0.6966\n",
            "[178] test loss: 10.808309555053711, accuracy: 0.6611\n",
            "[179] train loss: 40.5320348739624, accuracy: 0.6966\n",
            "[179] test loss: 10.807735919952393, accuracy: 0.6639\n",
            "[180] train loss: 40.52328181266785, accuracy: 0.6966\n",
            "[180] test loss: 10.813781499862671, accuracy: 0.6583\n",
            "[181] train loss: 40.517325043678284, accuracy: 0.6966\n",
            "[181] test loss: 10.811877608299255, accuracy: 0.6611\n",
            "[182] train loss: 40.517088294029236, accuracy: 0.6966\n",
            "[182] test loss: 10.814386487007141, accuracy: 0.6611\n",
            "[183] train loss: 40.515315651893616, accuracy: 0.6966\n",
            "[183] test loss: 10.812204122543335, accuracy: 0.6611\n",
            "[184] train loss: 40.5108939409256, accuracy: 0.6966\n",
            "[184] test loss: 10.808713555335999, accuracy: 0.6639\n",
            "[185] train loss: 40.50885498523712, accuracy: 0.6966\n",
            "[185] test loss: 10.80833113193512, accuracy: 0.6639\n",
            "[186] train loss: 40.50892996788025, accuracy: 0.6966\n",
            "[186] test loss: 10.809209108352661, accuracy: 0.6639\n",
            "[187] train loss: 40.508036971092224, accuracy: 0.6966\n",
            "[187] test loss: 10.806900978088379, accuracy: 0.6667\n",
            "[188] train loss: 40.507275462150574, accuracy: 0.6966\n",
            "[188] test loss: 10.79791784286499, accuracy: 0.6611\n",
            "[189] train loss: 40.50368785858154, accuracy: 0.6966\n",
            "[189] test loss: 10.799399137496948, accuracy: 0.6667\n",
            "[190] train loss: 40.499858379364014, accuracy: 0.6966\n",
            "[190] test loss: 10.795265793800354, accuracy: 0.6611\n",
            "[191] train loss: 40.498780846595764, accuracy: 0.6966\n",
            "[191] test loss: 10.796114206314087, accuracy: 0.6639\n",
            "[192] train loss: 40.497228503227234, accuracy: 0.6966\n",
            "[192] test loss: 10.790620803833008, accuracy: 0.6639\n",
            "[193] train loss: 40.44325613975525, accuracy: 0.6973\n",
            "[193] test loss: 10.657087206840515, accuracy: 0.7\n",
            "[194] train loss: 39.62049174308777, accuracy: 0.7495\n",
            "[194] test loss: 10.483307123184204, accuracy: 0.7222\n",
            "[195] train loss: 39.24516046047211, accuracy: 0.7613\n",
            "[195] test loss: 10.462445378303528, accuracy: 0.7278\n",
            "[196] train loss: 39.16947162151337, accuracy: 0.762\n",
            "[196] test loss: 10.488057255744934, accuracy: 0.7222\n",
            "[197] train loss: 39.196797251701355, accuracy: 0.7634\n",
            "[197] test loss: 10.502906918525696, accuracy: 0.7222\n",
            "[198] train loss: 39.395434617996216, accuracy: 0.7537\n",
            "[198] test loss: 10.354896545410156, accuracy: 0.7472\n",
            "[199] train loss: 39.331292152404785, accuracy: 0.7509\n",
            "[199] test loss: 10.367400050163269, accuracy: 0.7389\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOJkPf12jXV0",
        "colab_type": "text"
      },
      "source": [
        "## sklearn ensemble"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXBc8mUYfyKR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sklearn\n",
        "from sklearn.ensemble import BaggingClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFDYJEOopZgG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PytorchModel(sklearn.base.BaseEstimator):\n",
        "  def __init__(self, net_type, net_params, optim_type, optim_params, loss_fn,\n",
        "               input_shape, batch_size=32, accuracy_tol=0.02, tol_epochs=10,\n",
        "               cuda=True):\n",
        "    self.net_type = net_type\n",
        "    self.net_params = net_params\n",
        "    self.optim_type = optim_type\n",
        "    self.optim_params = optim_params\n",
        "    self.loss_fn = loss_fn\n",
        "\n",
        "    self.input_shape = input_shape\n",
        "    self.batch_size = batch_size\n",
        "    self.accuracy_tol = accuracy_tol\n",
        "    self.tol_epochs = tol_epochs\n",
        "    self.cuda = cuda\n",
        "  \n",
        "  def fit(self, X, y):\n",
        "    self.net = self.net_type(**self.net_params)\n",
        "    if self.cuda:\n",
        "      self.net = self.net.cuda()\n",
        "    self.optim = self.optim_type(self.net.parameters(), **self.optim_params)\n",
        "\n",
        "    uniq_classes = np.sort(np.unique(y))\n",
        "    self.classes_ = uniq_classes\n",
        "\n",
        "    X = X.reshape(-1, *self.input_shape)\n",
        "    x_tensor = torch.tensor(X.astype(np.float32))\n",
        "    y_tensor = torch.tensor(y.astype(np.long))\n",
        "    train_dataset = TensorDataset(x_tensor, y_tensor)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=self.batch_size,\n",
        "                              shuffle=True, drop_last=False)\n",
        "    last_accuracies = []\n",
        "    epoch = 0\n",
        "    keep_training = True\n",
        "    while keep_training:\n",
        "      self.net.train()\n",
        "      train_samples_count = 0\n",
        "      true_train_samples_count = 0\n",
        "\n",
        "      for batch in train_loader:\n",
        "        x_data, y_data = batch[0], batch[1]\n",
        "        if self.cuda:\n",
        "          x_data = x_data.cuda()\n",
        "          y_data = y_data.cuda()\n",
        "\n",
        "        y_pred = self.net(x_data)\n",
        "        loss = self.loss_fn(y_pred, y_data)\n",
        "\n",
        "        self.optim.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optim.step()\n",
        "\n",
        "        y_pred = y_pred.argmax(dim=1, keepdim=False)\n",
        "        true_classified = (y_pred == y_data).sum().item()\n",
        "        true_train_samples_count += true_classified\n",
        "        train_samples_count += len(x_data)\n",
        "      \n",
        "      train_accuracy = true_train_samples_count / train_samples_count\n",
        "      last_accuracies.append(train_accuracy)\n",
        "\n",
        "      if len(last_accuracies) > self.tol_epochs:\n",
        "        last_accuracies.pop(0)\n",
        "      \n",
        "      if len(last_accuracies) == self.tol_epochs:\n",
        "        accuracy_difference = max(last_accuracies) - min(last_accuracies)\n",
        "        if accuracy_difference <= self.accuracy_tol:\n",
        "          keep_training = False\n",
        "\n",
        "  def predict_proba(self, X, y=None):\n",
        "    X = X.reshape(-1, *self.input_shape)\n",
        "    x_tensor = torch.tensor(X.astype(np.float32))\n",
        "    if y:\n",
        "      y_tensor = torch.tensor(y.astype(np.long))\n",
        "    else:\n",
        "      y_tensor = torch.zeros(len(X), dtype=torch.long)\n",
        "    test_dataset = TensorDataset(x_tensor, y_tensor)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=self.batch_size,\n",
        "                              shuffle=False, drop_last=False)\n",
        "\n",
        "    self.net.eval()\n",
        "    predictions = []\n",
        "    for batch in test_loader:\n",
        "      x_data, y_data = batch[0], batch[1]\n",
        "      if self.cuda:\n",
        "        x_data = x_data.cuda()\n",
        "        y_data = y_data.cuda()\n",
        "\n",
        "      y_pred = self.net(x_data)\n",
        "      \n",
        "      predictions.append(y_pred.detach().cpu().numpy())\n",
        "    \n",
        "    predictions = np.concatenate(predictions)\n",
        "    return predictions\n",
        "  \n",
        "  def predict(self, X, y=None):\n",
        "    predictions = self.predict_proba(X, y)\n",
        "    predictions = predictions.argmax(axis=1)\n",
        "    return predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6V8VZlWrDm5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_model = PytorchModel(net_type=SimpleCNN, net_params=dict(), optim_type=Adam,\n",
        "                          optim_params={\"lr\": 1e-3}, loss_fn=nn.CrossEntropyLoss(),\n",
        "                          input_shape=(1, 8, 8), batch_size=32, accuracy_tol=0.02,\n",
        "                          tol_epochs=10, cuda=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eeCXhISuu2N9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "2bdada05-eb57-45e4-e33c-b02d7bdaa8b8"
      },
      "source": [
        "base_model.fit(x_train_scaled, y_train)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIJkwdm2u9s9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "6fa29e53-17c1-4366-834e-c3084aff4872"
      },
      "source": [
        "preds = base_model.predict(x_test_scaled)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmvn_t8FvDRE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8db7eed3-d8c4-4f36-d4c5-db825790067a"
      },
      "source": [
        "true_classified = (preds == y_test).sum()\n",
        "test_accuracy = true_classified / len(y_test)\n",
        "print(f\"Test accuracy: {test_accuracy}\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy: 0.7361111111111112\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqYwnvOzvkrJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "meta_classifier = BaggingClassifier(base_estimator=base_model, n_estimators=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2mVcFAGvt6Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "dd2339b3-dea6-4635-fa80-97a3c0533217"
      },
      "source": [
        "meta_classifier.fit(x_train_scaled.reshape(-1, 64), y_train)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BaggingClassifier(base_estimator=PytorchModel(accuracy_tol=0.02, batch_size=32,\n",
              "                                              cuda=True, input_shape=(1, 8, 8),\n",
              "                                              loss_fn=CrossEntropyLoss(),\n",
              "                                              net_params={},\n",
              "                                              net_type=<class '__main__.SimpleCNN'>,\n",
              "                                              optim_params={'lr': 0.001},\n",
              "                                              optim_type=<class 'torch.optim.adam.Adam'>,\n",
              "                                              tol_epochs=10),\n",
              "                  bootstrap=True, bootstrap_features=False, max_features=1.0,\n",
              "                  max_samples=1.0, n_estimators=10, n_jobs=None,\n",
              "                  oob_score=False, random_state=None, verbose=0,\n",
              "                  warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4S8gnY4x2He",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "4cb929cd-353c-4794-8e82-5fb675a4bfed"
      },
      "source": [
        "meta_classifier.score(x_test_scaled.reshape(-1, 64), y_test)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.95"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    }
  ]
}